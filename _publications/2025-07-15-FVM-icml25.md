---
title: "Towards Robust Influence Functions with Flat Validation Minima"
collection: publications
category: conference
permalink: /publication/2025-Flat-VM
date: 2025-07-15
venue: 'Forty-Second International Conference on Machine Learning (ICML)'
authors: "Xichen Ye, <strong>Yifan Wu</strong>, Weizhong Zhang<sup>#</sup>, Cheng Jin, Yifan Chen<sup>#</sup>"
paperurl: 'https://icml.cc/virtual/2025/poster/45098'
---
The Influence Function (IF) is a widely used technique for assessing the impact of individual training samples on model predictions.However, existing IF methods often fail to provide reliable influence estimates in deep neural networks, particularly when applied to noisy training data.This issue does not stem from inaccuracies in parameter change estimation, which has been the primary focus of prior research, but rather from deficiencies in loss change estimation, specifically due to the sharpness of validation risk.In this work, we establish a theoretical connection between influence estimation error, validation set risk, and its sharpness, underscoring the importance of flat validation minima for accurate influence estimation.Furthermore, we introduce a novel estimation form of Influence Function specifically designed for flat validation minima.Experimental results across various tasks validate the superiority of our approach.